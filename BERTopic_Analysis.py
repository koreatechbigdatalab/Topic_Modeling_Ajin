"""
BERTopic í† í”½ ëª¨ë¸ë§ ë¶„ì„ (TensorFlow ì—†ì´)
- Coherence ê³„ì‚°
- Topic Diversity ê³„ì‚°  
- Similarity Matrix ìƒì„±
- í† í”½ë³„ í‚¤ì›Œë“œ 10ê°œ, ê°€ì¤‘ì¹˜, ì˜ˆì‹œë¬¸ì¥ ì¶”ì¶œ

ğŸ¯ í† í”½ ìˆ˜ ì œì–´ ë°©ë²•:
1. ìë™ í† í”½ ìˆ˜ ê²°ì •: max_topics=None (ê¸°ë³¸ê°’, HDBSCAN ì‚¬ìš©)
2. ìµœëŒ€ í† í”½ ìˆ˜ ì œí•œ: max_topics=10 (HDBSCAN + reduce_topics)
3. ì •í™•í•œ í† í”½ ìˆ˜ ì§€ì •: run_analysis_with_fixed_topics(n_topics=8) (KMeans ì‚¬ìš©)

ğŸ“– ì‚¬ìš© ì˜ˆì‹œ:
# ë°©ë²• 1: ìë™ ê²°ì •
analyzer.run_full_analysis(data_path, max_topics=None)

# ë°©ë²• 2: ìµœëŒ€ 10ê°œë¡œ ì œí•œ
analyzer.run_full_analysis(data_path, max_topics=10)

# ë°©ë²• 3: ì •í™•íˆ 8ê°œ í† í”½ ìƒì„±
analyzer.run_analysis_with_fixed_topics(data_path, n_topics=8)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
from datetime import datetime
import pickle
from typing import List, Dict, Tuple, Any
import matplotlib.font_manager as fm

# TensorFlow í™˜ê²½ë³€ìˆ˜ ì„¤ì • (TensorFlow ë¹„í™œì„±í™”)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# ì§€ì—° ì„í¬íŠ¸ë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤
def import_bertopic():
    """BERTopic ëª¨ë“ˆì„ ì•ˆì „í•˜ê²Œ ì„í¬íŠ¸"""
    try:
        from bertopic import BERTopic
        return BERTopic
    except Exception as e:
        print(f"BERTopic ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ë³´ì„¸ìš”:")
        print("pip uninstall tensorflow")
        print("pip install bertopic sentence-transformers")
        raise

def import_sentence_transformer():
    """SentenceTransformerë¥¼ ì•ˆì „í•˜ê²Œ ì„í¬íŠ¸"""
    try:
        from sentence_transformers import SentenceTransformer
        return SentenceTransformer
    except Exception as e:
        print(f"SentenceTransformer ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        print("PyTorch ê¸°ë°˜ ì„¤ì¹˜ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”:")
        print("pip install sentence-transformers torch torchvision --index-url https://download.pytorch.org/whl/cpu")
        raise

def import_sklearn_modules():
    """scikit-learn ëª¨ë“ˆë“¤ì„ ì„í¬íŠ¸"""
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    return CountVectorizer, cosine_similarity

def import_umap_hdbscan():
    """UMAPê³¼ HDBSCANì„ ì„í¬íŠ¸"""
    from umap import UMAP
    from hdbscan import HDBSCAN
    return UMAP, HDBSCAN

def load_stopwords(stopwords_path='stopwords.txt'):
    """stopwords.txt íŒŒì¼ì—ì„œ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œ"""
    try:
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            stopwords = [line.strip() for line in f if line.strip()]
            print(f"[DEBUG] ë¶ˆìš©ì–´ ê°œìˆ˜: {len(stopwords)}")
            print(f"[DEBUG] ë¶ˆìš©ì–´ ì˜ˆì‹œ: {stopwords[:10]}")
            return stopwords
    except Exception as e:
        print(f"ë¶ˆìš©ì–´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    plt.rcParams['font.family'] = 'AppleGothic'
except:
    try:
        plt.rcParams['font.family'] = 'AppleGothic'
    except:
        plt.rcParams['font.family'] = 'DejaVu Sans'
        warnings.warn("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
plt.rcParams['axes.unicode_minus'] = False

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

class BERTopicAnalyzer:
    """BERTopicì„ ì´ìš©í•œ í† í”½ ëª¨ë¸ë§ ë¶„ì„ í´ë˜ìŠ¤ (TensorFlow ì—†ì´)"""
    
    def __init__(self, language_model: str = "jhgan/ko-sroberta-multitask"):
        """
        BERTopic ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            language_model: í•œêµ­ì–´ ì„ë² ë”©ì„ ìœ„í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸
        """
        self.language_model = language_model
        self.embedding_model = None
        self.topic_model = None
        self.documents = None
        self.embeddings = None
        self.topics = None
        self.probabilities = None
        self.results = {}
        
    def load_data(self, file_path: str, text_column: str = 'cleaned_text') -> pd.DataFrame:
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
        
        Args:
            file_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ (pre_dataframe.xlsx)
            text_column: í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì»¬ëŸ¼ëª… (cleaned_text)
            
        Returns:
            ë¡œë“œëœ DataFrame
        """
        try:
            print(f"ë°ì´í„° ë¡œë”© ì¤‘: {file_path}")
            
            if file_path.endswith('.xlsx'):
                df = pd.read_excel(file_path)
            elif file_path.endswith('.csv'):
                df = pd.read_csv(file_path, encoding='utf-8')
            else:
                raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. .xlsx ë˜ëŠ” .csv íŒŒì¼ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
                
            print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ë¬¸ì„œ")
            print(f"ì»¬ëŸ¼: {list(df.columns)}")
            
            # cleaned_text ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if text_column not in df.columns:
                print(f"âŒ '{text_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
                raise ValueError(f"'{text_column}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. pre_dataframe.xlsx íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            
            # ë¹ˆ ê°’ ì œê±°
            original_count = len(df)
            df = df.dropna(subset=[text_column])
            df = df[df[text_column].str.strip() != '']
            
            print(f"ë¹ˆ ê°’ ì œê±° í›„: {len(df)}ê°œ ë¬¸ì„œ (ì œê±°ëœ ë¬¸ì„œ: {original_count - len(df)}ê°œ)")
            
            if len(df) == 0:
                raise ValueError("ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            self.documents = df[text_column].tolist()
            print(f"âœ… ë¶„ì„ ëŒ€ìƒ ë¬¸ì„œ ìˆ˜: {len(self.documents)}")
            
            return df
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def setup_model(self, 
                   min_topic_size: int = 10,
                   n_neighbors: int = 15,
                   n_components: int = 5,
                   min_cluster_size: int = 10,
                   metric: str = 'euclidean',
                   cluster_selection_method: str = 'eom',
                   max_topics: int = None,
                   random_state: int = 42) -> None:
        """
        BERTopic ëª¨ë¸ ì„¤ì •
        
        Args:
            max_topics: ìµœëŒ€ í† í”½ ìˆ˜ ì œí•œ (Noneì´ë©´ ìë™ ê²°ì •)
        """
        print("ğŸ”§ ëª¨ë¸ ì„¤ì • ì¤‘...")
        
        # max_topics ì €ì¥ (ë‚˜ì¤‘ì— reduce_topicsì—ì„œ ì‚¬ìš©)
        self.max_topics = max_topics
        
        try:
            # í•„ìš”í•œ ëª¨ë“ˆë“¤ ë™ì  ì„í¬íŠ¸
            BERTopic = import_bertopic()
            SentenceTransformer = import_sentence_transformer()
            CountVectorizer, cosine_similarity = import_sklearn_modules()
            UMAP, HDBSCAN = import_umap_hdbscan()
            
            # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
            print(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.language_model}")
            self.embedding_model = SentenceTransformer(self.language_model, device='cpu')
            
            # UMAP ì°¨ì› ì¶•ì†Œ ì„¤ì •
            umap_model = UMAP(
                n_neighbors=n_neighbors,
                n_components=n_components,
                min_dist=0.0,
                metric=metric,
                random_state=random_state
            )
            
            # HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •
            hdbscan_model = HDBSCAN(
                min_cluster_size=min_cluster_size,
                metric='euclidean',
                cluster_selection_method=cluster_selection_method,
                prediction_data=True
            )
            
            # CountVectorizer ì„¤ì • (í•œêµ­ì–´ ì²˜ë¦¬)
            stopwords = load_stopwords('stopwords.txt')
            if stopwords is None:
                print("[WARNING] stopwords.txtë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¶ˆìš©ì–´ê°€ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                print(f"[DEBUG] CountVectorizerì— ì ìš©ëœ ë¶ˆìš©ì–´ ê°œìˆ˜: {len(stopwords)}")
            vectorizer_model = CountVectorizer(
                ngram_range=(1, 2),
                stop_words=stopwords,
                min_df=2,
                max_df=0.95
            )
            
            # BERTopic ëª¨ë¸ ìƒì„±
            self.topic_model = BERTopic(
                embedding_model=self.embedding_model,
                umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                vectorizer_model=vectorizer_model,
                language='korean',
                calculate_probabilities=True,
                verbose=True
            )
            
            print("âœ… ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
            if max_topics:
                print(f"ğŸ“Š ìµœëŒ€ í† í”½ ìˆ˜ ì œí•œ: {max_topics}ê°œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("\ní•´ê²° ë°©ë²•:")
            print("1. pip uninstall tensorflow tensorflow-gpu")
            print("2. pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu")
            print("3. pip install bertopic sentence-transformers")
            raise
    
    def setup_model_with_kmeans(self, 
                               n_topics: int,
                               n_neighbors: int = 15,
                               n_components: int = 5,
                               random_state: int = 42) -> None:
        """
        KMeansë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ í† í”½ ìˆ˜ë¥¼ ì§€ì •í•˜ëŠ” BERTopic ëª¨ë¸ ì„¤ì •
        
        Args:
            n_topics: ìƒì„±í•  í† í”½ ìˆ˜ (ì •í™•íˆ ì´ ê°œìˆ˜ë§Œí¼ ìƒì„±ë¨)
            n_neighbors: UMAP n_neighbors íŒŒë¼ë¯¸í„°
            n_components: UMAP ì°¨ì› ìˆ˜
            random_state: ëœë¤ ì‹œë“œ
        """
        print(f"ğŸ”§ KMeans ê¸°ë°˜ ëª¨ë¸ ì„¤ì • ì¤‘... (í† í”½ ìˆ˜: {n_topics})")
        
        try:
            # í•„ìš”í•œ ëª¨ë“ˆë“¤ ë™ì  ì„í¬íŠ¸
            BERTopic = import_bertopic()
            SentenceTransformer = import_sentence_transformer()
            CountVectorizer, cosine_similarity = import_sklearn_modules()
            UMAP, _ = import_umap_hdbscan()
            
            from sklearn.cluster import KMeans
            
            # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
            print(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.language_model}")
            self.embedding_model = SentenceTransformer(self.language_model, device='cpu')
            
            # UMAP ì°¨ì› ì¶•ì†Œ ì„¤ì •
            umap_model = UMAP(
                n_neighbors=n_neighbors,
                n_components=n_components,
                min_dist=0.0,
                metric='cosine',
                random_state=random_state
            )
            
            # KMeans í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì • (ì •í™•í•œ í† í”½ ìˆ˜ ì§€ì •)
            kmeans_model = KMeans(
                n_clusters=n_topics,
                random_state=random_state,
                n_init=10
            )
            
            # CountVectorizer ì„¤ì • (í•œêµ­ì–´ ì²˜ë¦¬)
            stopwords = load_stopwords('stopwords.txt')
            if stopwords is None:
                print("[WARNING] stopwords.txtë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¶ˆìš©ì–´ê°€ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                print(f"[DEBUG] CountVectorizerì— ì ìš©ëœ ë¶ˆìš©ì–´ ê°œìˆ˜: {len(stopwords)}")
            vectorizer_model = CountVectorizer(
                ngram_range=(1, 2),
                stop_words=stopwords,
                min_df=2,
                max_df=0.95
            )
            
            # BERTopic ëª¨ë¸ ìƒì„± (KMeans ì‚¬ìš©)
            self.topic_model = BERTopic(
                embedding_model=self.embedding_model,
                umap_model=umap_model,
                hdbscan_model=kmeans_model,  # KMeansë¥¼ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì‚¬ìš©
                vectorizer_model=vectorizer_model,
                language='korean',
                calculate_probabilities=False,  # KMeansëŠ” í™•ë¥  ê³„ì‚° ì•ˆ í•¨
                verbose=True
            )
            
            print(f"âœ… KMeans ê¸°ë°˜ ëª¨ë¸ ì„¤ì • ì™„ë£Œ (ì •í™•íˆ {n_topics}ê°œ í† í”½ ìƒì„±)")
            
        except Exception as e:
            print(f"âŒ KMeans ëª¨ë¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def fit_transform(self) -> Tuple[List[int], np.ndarray]:
        """
        BERTopic ëª¨ë¸ í•™ìŠµ ë° í† í”½ í• ë‹¹
        """
        print("ğŸš€ BERTopic ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        if self.documents is None:
            raise ValueError("ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        
        if self.topic_model is None:
            self.setup_model()
        
        try:
            # í† í”½ ëª¨ë¸ë§ ì‹¤í–‰
            self.topics, self.probabilities = self.topic_model.fit_transform(self.documents)
            
            unique_topics = len(set(self.topics)) - (1 if -1 in self.topics else 0)
            noise_docs = sum(1 for topic in self.topics if topic == -1)
            
            print(f"âœ… ì´ˆê¸° í† í”½ ëª¨ë¸ë§ ì™„ë£Œ!")
            print(f"   - ë°œê²¬ëœ í† í”½ ìˆ˜: {unique_topics}")
            print(f"   - ë…¸ì´ì¦ˆ ë¬¸ì„œ ìˆ˜: {noise_docs}")
            
            # max_topicsê°€ ì§€ì •ëœ ê²½ìš° í† í”½ ìˆ˜ ì¤„ì´ê¸°
            if hasattr(self, 'max_topics') and self.max_topics and unique_topics > self.max_topics:
                print(f"ğŸ”„ í† í”½ ìˆ˜ë¥¼ {self.max_topics - 1}ê°œë¡œ ì¤„ì´ëŠ” ì¤‘...")
                
                # í† í”½ ìˆ˜ ì¤„ì´ê¸°
                self.topic_model.reduce_topics(self.documents, nr_topics=self.max_topics)
                
                # ìƒˆë¡œìš´ í† í”½ í• ë‹¹ ì–»ê¸°
                self.topics = self.topic_model.transform(self.documents)[0]
                
                final_unique_topics = len(set(self.topics)) - (1 if -1 in self.topics else 0)
                final_noise_docs = sum(1 for topic in self.topics if topic == -1)
                
                print(f"âœ… í† í”½ ìˆ˜ ì¡°ì • ì™„ë£Œ!")
                print(f"   - ìµœì¢… í† í”½ ìˆ˜: {final_unique_topics}")
                print(f"   - ë…¸ì´ì¦ˆ ë¬¸ì„œ ìˆ˜: {final_noise_docs}")
            
            return self.topics, self.probabilities
            
        except Exception as e:
            print(f"âŒ í† í”½ ëª¨ë¸ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def calculate_coherence(self) -> float:
        """
        í† í”½ ì¼ê´€ì„±(Coherence) ê³„ì‚°
        """
        try:
            print("ğŸ“Š Coherence ê³„ì‚° ì¤‘...")
            
            try:
                from gensim.models import CoherenceModel
                from gensim.corpora import Dictionary
            except ImportError:
                print("âš ï¸ gensimì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Coherence ê³„ì‚°ì„ ê±´ë„ˆë›°ê² ìŠµë‹ˆë‹¤.")
                print("ì„¤ì¹˜: pip install gensim")
                return None
            
            # í† í”½ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
            topic_words = []
            topic_info = self.topic_model.get_topic_info()
            
            for topic_id in topic_info['Topic'].values:
                if topic_id != -1:  # ë…¸ì´ì¦ˆ í† í”½ ì œì™¸
                    words = [word for word, _ in self.topic_model.get_topic(topic_id)]
                    topic_words.append(words)
            
            if len(topic_words) == 0:
                print("âš ï¸ ìœ íš¨í•œ í† í”½ì´ ì—†ì–´ì„œ Coherenceë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ë¬¸ì„œë¥¼ í† í°ìœ¼ë¡œ ë¶„í• 
            texts = [doc.split() for doc in self.documents]
            
            # Gensim Dictionary ìƒì„±
            dictionary = Dictionary(texts)
            
            # Coherence ëª¨ë¸ ìƒì„±
            coherence_model = CoherenceModel(
                topics=topic_words,
                texts=texts,
                dictionary=dictionary,
                coherence='c_v'
            )
            
            coherence_score = coherence_model.get_coherence()
            self.results['coherence'] = coherence_score
            
            print(f"âœ… Coherence Score (C_V): {coherence_score:.4f}")
            return coherence_score
            
        except Exception as e:
            print(f"âŒ Coherence ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def calculate_topic_diversity(self) -> float:
        """
        í† í”½ ë‹¤ì–‘ì„±(Topic Diversity) ê³„ì‚°
        """
        print("ğŸ“Š Topic Diversity ê³„ì‚° ì¤‘...")
        
        try:
            topic_info = self.topic_model.get_topic_info()
            all_words = set()
            total_words = 0
            
            for topic_id in topic_info['Topic'].values:
                if topic_id != -1:  # ë…¸ì´ì¦ˆ í† í”½ ì œì™¸
                    words = [word for word, _ in self.topic_model.get_topic(topic_id)]
                    all_words.update(words)
                    total_words += len(words)
            
            if total_words == 0:
                return 0.0
            
            diversity_score = len(all_words) / total_words
            self.results['topic_diversity'] = diversity_score
            
            print(f"âœ… Topic Diversity Score: {diversity_score:.4f}")
            print(f"   - ì´ ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜: {len(all_words)}")
            print(f"   - ì´ í‚¤ì›Œë“œ ìˆ˜: {total_words}")
            
            return diversity_score
            
        except Exception as e:
            print(f"âŒ Topic Diversity ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def create_similarity_matrix(self) -> np.ndarray:
        """
        í† í”½ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        """
        print("ğŸ“Š Similarity Matrix ìƒì„± ì¤‘...")
        
        try:
            # sklearn ëª¨ë“ˆ ì„í¬íŠ¸
            _, cosine_similarity = import_sklearn_modules()
            
            # í† í”½ ì„ë² ë”© ì¶”ì¶œ
            topic_embeddings = []
            topic_info = self.topic_model.get_topic_info()
            
            for topic_id in topic_info['Topic'].values:
                if topic_id != -1:  # ë…¸ì´ì¦ˆ í† í”½ ì œì™¸
                    topic_words = [word for word, _ in self.topic_model.get_topic(topic_id)]
                    # í† í”½ì˜ í‚¤ì›Œë“œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê²°í•©
                    topic_text = ' '.join(topic_words)
                    embedding = self.embedding_model.encode([topic_text])
                    topic_embeddings.append(embedding[0])
            
            if len(topic_embeddings) == 0:
                print("âš ï¸ ìœ íš¨í•œ í† í”½ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            topic_embeddings = np.array(topic_embeddings)
            similarity_matrix = cosine_similarity(topic_embeddings)
            
            self.results['similarity_matrix'] = similarity_matrix
            
            # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì‹œê°í™”
            self.plot_similarity_matrix(similarity_matrix)
            
            print(f"âœ… Similarity Matrix ìƒì„± ì™„ë£Œ: {similarity_matrix.shape}")
            return similarity_matrix
            
        except Exception as e:
            print(f"âŒ Similarity Matrix ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def plot_similarity_matrix(self, similarity_matrix: np.ndarray) -> None:
        """
        ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì‹œê°í™”
        """
        try:
            plt.figure(figsize=(12, 10))
            
            # í† í”½ ë¼ë²¨ ìƒì„±
            topic_info = self.topic_model.get_topic_info()
            valid_topics = [t for t in topic_info['Topic'].values if t != -1]
            labels = [f'Topic {i}' for i in valid_topics]
            
            # íˆíŠ¸ë§µ ìƒì„±
            sns.heatmap(
                similarity_matrix,
                annot=False,  # ì ìˆ˜ í‘œì‹œ ì•ˆí•¨
                cmap='viridis',
                xticklabels=labels,
                yticklabels=labels,
                cbar_kws={'label': 'Cosine Similarity'}
            )
            
            plt.title('Similarrity Matrix', fontsize=16, pad=20)
            plt.xlabel('Topics', fontsize=12)
            plt.ylabel('Topics', fontsize=12)
            plt.tight_layout()
            
            # ê²°ê³¼ í´ë”ì— ì €ì¥
            os.makedirs('Results', exist_ok=True)
            plt.savefig(f'Results/BERTopic_similarity_matrix_max{self.max_topics - 1}.png', dpi=300, bbox_inches='tight')
            # plt.show()
            
            print(f"âœ… ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì‹œê°í™” ì™„ë£Œ: Results/BERTopic_similarity_matrix_max{self.max_topics - 1}.png")
            
        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def extract_topic_keywords_and_examples(self, top_k: int = 10) -> Dict[int, Dict]:
        """
        ê° í† í”½ë³„ í‚¤ì›Œë“œ, ê°€ì¤‘ì¹˜, ì˜ˆì‹œë¬¸ì¥ ì¶”ì¶œ (ì‹¤ì œ í™•ë¥ /ìœ ì‚¬ë„ ê°’ ê³„ì‚°)
        """
        print(f"ğŸ“Š í† í”½ë³„ í‚¤ì›Œë“œ {top_k}ê°œ, ê°€ì¤‘ì¹˜, ì˜ˆì‹œë¬¸ì¥ ì¶”ì¶œ ì¤‘...")
        
        try:
            topic_details = {}
            topic_info = self.topic_model.get_topic_info()
            
            for topic_id in topic_info['Topic'].values:
                if topic_id != -1:  # ë…¸ì´ì¦ˆ í† í”½ ì œì™¸
                    # í‚¤ì›Œë“œì™€ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
                    topic_words = self.topic_model.get_topic(topic_id)
                    keywords = []
                    weights = []
                    
                    for word, weight in topic_words[:top_k]:
                        keywords.append(word)
                        weights.append(weight)
                    
                    # í•´ë‹¹ í† í”½ì˜ ë¬¸ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°
                    topic_docs_indices = [i for i, topic in enumerate(self.topics) if topic == topic_id]
                    
                    # ì˜ˆì‹œ ë¬¸ì¥ ì¶”ì¶œ (ì‹¤ì œ í™•ë¥ /ìœ ì‚¬ë„ ê°’ ê³„ì‚°)
                    if topic_docs_indices:
                        doc_probs = []
                        
                        if self.probabilities is not None:
                            # HDBSCANì˜ ê²½ìš° - ì‹¤ì œ í™•ë¥  ì‚¬ìš©
                            print(f"   Topic {topic_id}: HDBSCAN í™•ë¥ ê°’ ì‚¬ìš©")
                            for i in topic_docs_indices:
                                prob = float(self.probabilities[i][topic_id]) if hasattr(self.probabilities[i], '__getitem__') else float(self.probabilities[i])
                                doc_probs.append((i, prob))
                        
                        elif hasattr(self, 'embedding_model') and self.embedding_model is not None:
                            # KMeansì˜ ê²½ìš° - í† í”½ ì¤‘ì‹¬ê³¼ ë¬¸ì„œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                            print(f"   Topic {topic_id}: ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
                            topic_words_list = [word for word, _ in self.topic_model.get_topic(topic_id)]
                            topic_text = ' '.join(topic_words_list[:5])  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
                            topic_embedding = self.embedding_model.encode([topic_text])[0]
                            
                            for i in topic_docs_indices:
                                doc_embedding = self.embedding_model.encode([self.documents[i]])[0]
                                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì •ê·œí™”ëœ 0~1 ë²”ìœ„)
                                similarity = np.dot(topic_embedding, doc_embedding) / (
                                    np.linalg.norm(topic_embedding) * np.linalg.norm(doc_embedding)
                                )
                                # -1~1 ë²”ìœ„ë¥¼ 0~1ë¡œ ì •ê·œí™”
                                normalized_similarity = (similarity + 1) / 2
                                doc_probs.append((i, float(normalized_similarity)))
                        
                        else:
                            # ì„ë² ë”© ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° - TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
                            print(f"   Topic {topic_id}: TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
                            try:
                                from sklearn.feature_extraction.text import TfidfVectorizer
                                from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity
                                
                                # í† í”½ í‚¤ì›Œë“œë¡œ ëŒ€í‘œ ë¬¸ì„œ ìƒì„±
                                topic_words_list = [word for word, _ in self.topic_model.get_topic(topic_id)]
                                topic_repr = ' '.join(topic_words_list[:10])
                                
                                # í•´ë‹¹ í† í”½ì˜ ë¬¸ì„œë“¤ê³¼ í† í”½ ëŒ€í‘œ ë¬¸ì„œ ê²°í•©
                                docs_for_comparison = [topic_repr] + [self.documents[i] for i in topic_docs_indices]
                                
                                # TF-IDF ë²¡í„°í™”
                                tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
                                tfidf_matrix = tfidf.fit_transform(docs_for_comparison)
                                
                                # í† í”½ ëŒ€í‘œ ë¬¸ì„œ(ì²« ë²ˆì§¸)ì™€ ê° ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
                                topic_vector = tfidf_matrix[0:1]  # ì²« ë²ˆì§¸ ë¬¸ì„œ (í† í”½ ëŒ€í‘œ)
                                doc_vectors = tfidf_matrix[1:]     # ë‚˜ë¨¸ì§€ ë¬¸ì„œë“¤
                                
                                similarities = sklearn_cosine_similarity(topic_vector, doc_vectors)[0]
                                
                                for idx, similarity in enumerate(similarities):
                                    doc_index = topic_docs_indices[idx]
                                    doc_probs.append((doc_index, float(similarity)))
                                    
                            except ImportError:
                                # sklearnì´ ì—†ëŠ” ê²½ìš° ë¬¸ì„œ ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
                                print(f"   Topic {topic_id}: ë¬¸ì„œ ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ ì‚¬ìš©")
                                topic_words_set = set([word for word, _ in self.topic_model.get_topic(topic_id)])
                                
                                for i in topic_docs_indices:
                                    doc_words = set(self.documents[i].split())
                                    # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
                                    intersection = len(topic_words_set.intersection(doc_words))
                                    union = len(topic_words_set.union(doc_words))
                                    jaccard_similarity = intersection / union if union > 0 else 0.0
                                    doc_probs.append((i, float(jaccard_similarity)))
                        
                        # í™•ë¥ /ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                        doc_probs.sort(key=lambda x: x[1], reverse=True)
                        
                        # í†µê³„ ì •ë³´ ì¶œë ¥
                        if doc_probs:
                            prob_values = [prob for _, prob in doc_probs]
                            print(f"     ë¬¸ì„œ ìˆ˜: {len(doc_probs)}, í™•ë¥ /ìœ ì‚¬ë„ ë²”ìœ„: {min(prob_values):.4f} ~ {max(prob_values):.4f}")
                        
                        # ìƒìœ„ 3ê°œ ë¬¸ì„œë¥¼ ì˜ˆì‹œë¡œ ì„ íƒ
                        example_docs = []
                        for i, prob in doc_probs[:3]:
                            example_docs.append({
                                'text': self.documents[i],
                                'probability': prob,
                                'probability_type': self._get_probability_type()
                            })
                    else:
                        example_docs = []
                    
                    topic_details[topic_id] = {
                        'keywords': keywords,
                        'weights': weights,
                        'example_documents': example_docs,
                        'document_count': len(topic_docs_indices)
                    }
            
            self.results['topic_details'] = topic_details
            
            print(f"âœ… í† í”½ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {len(topic_details)}ê°œ í† í”½")
            return topic_details
            
        except Exception as e:
            print(f"âŒ í† í”½ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}
    
    def _get_probability_type(self) -> str:
        """í˜„ì¬ í™•ë¥  ê³„ì‚° ë°©ì‹ì„ ë°˜í™˜"""
        if self.probabilities is not None:
            return "HDBSCAN_probability"
        elif hasattr(self, 'embedding_model') and self.embedding_model is not None:
            return "embedding_cosine_similarity"
        else:
            return "tfidf_similarity"
    
    def save_results(self, output_dir: str = 'Results', top_k: int = 10) -> None:
        """
        ê²°ê³¼ ì €ì¥ ê´€ë¦¬ í•¨ìˆ˜ (ì—‘ì…€, ì‹œê°í™” ë“±)
        """
        import pandas as pd
        import os
        from datetime import datetime
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        excel_path = os.path.join(output_dir, f'bertopic_analysis_results_{timestamp}.xlsx')

        # ë¶ˆìš©ì–´ ë¡œë“œ
        stopwords = set(load_stopwords('stopwords.txt'))

        # í† í”½ë³„ í‚¤ì›Œë“œì™€ ê°€ì¤‘ì¹˜ ì €ì¥
        topic_info = self.topic_model.get_topic_info()
        used_words = set()
        keywords_data = []

        for topic_id in topic_info['Topic'].values:
            if topic_id == -1:
                continue
            topic_words = self.topic_model.get_topic(topic_id)
            unique_words = []
            for word, weight in topic_words:
                # ì™„ì „ì¼ì¹˜ ë˜ëŠ” ë¶ˆìš©ì–´ê°€ ë‹¨ì–´ ë‚´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì œê±°
                if word in stopwords:
                    continue
                if any(sw in word for sw in stopwords):
                    continue
                if word not in used_words:
                    unique_words.append((word, weight))
                    used_words.add(word)
                if len(unique_words) >= top_k:
                    break
            for rank, (word, weight) in enumerate(unique_words, 1):
                keywords_data.append({
                    'Topic': topic_id,
                    'Keyword Rank': rank,
                    'Keyword': word,
                    'Weight': weight
                })

        # DataFrameìœ¼ë¡œ ë³€í™˜ (ì—‘ì…€ ì €ì¥ ë“±)
        keywords_df = pd.DataFrame(keywords_data)
        keywords_df.to_excel(f'{output_dir}/ì¤‘ë³µì—†ëŠ”_í† í”½í‚¤ì›Œë“œ.xlsx', index=False)

        # ê¸°ì¡´ ê²°ê³¼ ì €ì¥ ë¡œì§
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # í† í”½ë³„ í‚¤ì›Œë“œ/ê°€ì¤‘ì¹˜ ì‹œíŠ¸
            keywords_df.to_excel(writer, sheet_name='Keywords', index=False)
            # ê¸°ì¡´ summary, examples ë“± ë‹¤ë¥¸ ì‹œíŠ¸ë„ í•„ìš”ì‹œ ì¶”ê°€
        print(f"âœ… í† í”½ë³„ í‚¤ì›Œë“œ/ê°€ì¤‘ì¹˜ ì—‘ì…€ ì €ì¥: {excel_path}")

    def create_visualizations(self, output_dir: str = 'Results') -> None:
        """
        í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ì‹œê°í™” ìƒì„±
        """
        print("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ì „ì œ ì¡°ê±´ í™•ì¸ (ì™„í™”ëœ ë²„ì „)
        if self.topic_model is None:
            print("âŒ í† í”½ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € fit_transform()ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        # í† í”½ í• ë‹¹ì´ ì—†ì–´ë„ ëª¨ë¸ ê¸°ë°˜ ì‹œê°í™”ëŠ” ê°€ëŠ¥
        if self.topics is None:
            print("âš ï¸ í† í”½ í• ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ê¸°ë°˜ ì‹œê°í™”ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        try:
            os.makedirs(output_dir, exist_ok=True)
            print(f"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±/í™•ì¸ ì™„ë£Œ: {output_dir}")
            
            # í† í”½ ì •ë³´ í™•ì¸
            topic_info = self.topic_model.get_topic_info()
            valid_topics = len(topic_info) - 1 if -1 in topic_info['Topic'].values else len(topic_info)
            print(f"ğŸ“Š ì‹œê°í™”í•  í† í”½ ìˆ˜: {valid_topics}")
            
            if valid_topics == 0:
                print("âš ï¸ ìœ íš¨í•œ í† í”½ì´ ì—†ì–´ì„œ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            visualizations_created = []
            
            # 1. í† í”½ ë¶„í¬ ì‹œê°í™” (ê¸°ë³¸)
            try:
                print("   1. í† í”½ ë¶„í¬ ì‹œê°í™” ìƒì„± ì¤‘...")
                fig1 = self.topic_model.visualize_barchart(top_n_topics=valid_topics)  # API ìˆ˜ì •
                fig1.write_html(f'{output_dir}/BERTopic_topic_barchart.html')
                visualizations_created.append("í† í”½ ë¶„í¬ ì°¨íŠ¸ (barchart)")
                print("   âœ… í† í”½ ë¶„í¬ ì‹œê°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ í† í”½ ë¶„í¬ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            
            # 2. Topic Word Scores ì‹œê°í™” (ê°œì„ ëœ ë²„ì „)
            try:
                print("   2. í† í”½ í‚¤ì›Œë“œ ì ìˆ˜ ì‹œê°í™” ìƒì„± ì¤‘...")
                fig2 = self.topic_model.visualize_barchart(
                    top_n_topics=valid_topics,  # API ìˆ˜ì •: top_nr_topics â†’ top_n_topics
                    n_words=5,  # ê° í† í”½ë‹¹ ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
                    title="Topic Word Scores"
                )
                fig2.write_html(f'{output_dir}/BERTopic_topic_word_scores.html')
                visualizations_created.append("í† í”½ í‚¤ì›Œë“œ ì ìˆ˜ ì°¨íŠ¸")
                print("   âœ… í† í”½ í‚¤ì›Œë“œ ì ìˆ˜ ì‹œê°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ í† í”½ í‚¤ì›Œë“œ ì ìˆ˜ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            
            # 3. í† í”½ ê°„ ê±°ë¦¬ ì‹œê°í™”
            try:
                print("   3. í† í”½ ê°„ ê±°ë¦¬ ì‹œê°í™” ìƒì„± ì¤‘...")
                fig3 = self.topic_model.visualize_topics()
                fig3.write_html(f'{output_dir}/BERTopic_topic_distance.html')
                visualizations_created.append("í† í”½ ê°„ ê±°ë¦¬ ì‹œê°í™”")
                print("   âœ… í† í”½ ê°„ ê±°ë¦¬ ì‹œê°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ í† í”½ ê°„ ê±°ë¦¬ ì‹œê°í™” ì‹¤íŒ¨: {e}")
                print(f"      ì›ì¸: UMAP ì°¨ì›ì¶•ì†Œê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # 4. ê³„ì¸µì  í† í”½ ì‹œê°í™”
            try:
                print("   4. ê³„ì¸µì  í† í”½ ì‹œê°í™” ìƒì„± ì¤‘...")
                fig4 = self.topic_model.visualize_hierarchy()
                fig4.write_html(f'{output_dir}/BERTopic_topic_hierarchy.html')
                visualizations_created.append("ê³„ì¸µì  í† í”½ êµ¬ì¡°")
                print("   âœ… ê³„ì¸µì  í† í”½ ì‹œê°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ ê³„ì¸µì  í† í”½ ì‹œê°í™” ì‹¤íŒ¨: {e}")
                print(f"      ì›ì¸: í† í”½ ìˆ˜ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ê³„ì¸µ êµ¬ì¡° ê³„ì‚°ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # 5. í† í”½ë³„ íˆíŠ¸ë§µ
            try:
                print("   5. í† í”½ íˆíŠ¸ë§µ ì‹œê°í™” ìƒì„± ì¤‘...")
                fig5 = self.topic_model.visualize_heatmap()
                fig5.write_html(f'{output_dir}/BERTopic_topic_heatmap.html')
                visualizations_created.append("í† í”½ íˆíŠ¸ë§µ")
                print("   âœ… í† í”½ íˆíŠ¸ë§µ ì‹œê°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"   âŒ í† í”½ íˆíŠ¸ë§µ ì‹œê°í™” ì‹¤íŒ¨: {e}")
                print(f"      ì›ì¸: í† í”½ ê°„ ìœ ì‚¬ë„ ê³„ì‚°ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # ê²°ê³¼ ìš”ì•½
            if visualizations_created:
                print(f"\nâœ… ì‹œê°í™” íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_dir}/")
                print("ğŸ“‹ ìƒì„±ëœ ì‹œê°í™”:")
                for viz in visualizations_created:
                    print(f"   - {viz}")
            else:
                print("âŒ ì‹œê°í™” íŒŒì¼ì„ í•˜ë‚˜ë„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ ì‹œê°í™” ìƒì„± ì¤‘ ì „ì²´ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ğŸ”§ í•´ê²° ë°©ë²•:")
            print("  1. plotly ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—…ë°ì´íŠ¸: pip install --upgrade plotly")
            print("  2. BERTopic ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆëŠ”ì§€ í™•ì¸")
            print("  3. ì¶©ë¶„í•œ í† í”½ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸")
            import traceback
            print(f"\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\n{traceback.format_exc()}")
    
    def create_safe_visualizations(self, output_dir: str = 'Results') -> None:
        """
        ì•ˆì „í•œ ìµœì†Œ ì‹œê°í™” ìƒì„± (ê°€ì¥ ê¸°ë³¸ì ì¸ ê²ƒë“¤ë§Œ)
        """
        print("ğŸ“Š ì•ˆì „í•œ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        if self.topic_model is None:
            print("âŒ í† í”½ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            # ê°€ì¥ ê¸°ë³¸ì ì¸ í† í”½ ë¶„í¬ ì°¨íŠ¸ë§Œ ìƒì„± (API ìˆ˜ì •)
            print("   í† í”½ ë¶„í¬ ì°¨íŠ¸ ìƒì„± ì¤‘...")
            fig = self.topic_model.visualize_barchart(top_n_topics=5)  # top_nr_topics â†’ top_n_topics
            fig.write_html(f'{output_dir}/safe_topic_barchart.html')
            print("   âœ… ê¸°ë³¸ í† í”½ ë¶„í¬ ì°¨íŠ¸ ì™„ë£Œ")
            
            # í† í”½ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
            topic_info = self.topic_model.get_topic_info()
            with open(f'{output_dir}/topic_summary.txt', 'w', encoding='utf-8') as f:
                f.write("í† í”½ ìš”ì•½ ì •ë³´\n")
                f.write("=" * 30 + "\n\n")
                for _, row in topic_info.iterrows():
                    if row['Topic'] != -1:
                        f.write(f"í† í”½ {row['Topic']}: {row['Count']}ê°œ ë¬¸ì„œ\n")
                        # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
                        keywords = [word for word, _ in self.topic_model.get_topic(row['Topic'])[:5]]
                        f.write(f"í‚¤ì›Œë“œ: {', '.join(keywords)}\n\n")
            
            print("   âœ… í† í”½ ìš”ì•½ í…ìŠ¤íŠ¸ íŒŒì¼ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì•ˆì „í•œ ì‹œê°í™”ë„ ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
    
    def run_full_analysis(self, data_path: str, text_column: str = 'cleaned_text', max_topics: int = None) -> Dict[str, Any]:
        """
        ì „ì²´ BERTopic ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (pre_dataframe.xlsxì˜ cleaned_text ì»¬ëŸ¼ ì‚¬ìš©)
        
        Args:
            data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
            max_topics: ìµœëŒ€ í† í”½ ìˆ˜ ì œí•œ (Noneì´ë©´ ìë™ ê²°ì •)
        """
        print("ğŸš€ BERTopic ì „ì²´ ë¶„ì„ ì‹œì‘...")
        print("=" * 50)
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            df = self.load_data(data_path, text_column)
            
            # 2. ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ
            self.setup_model(max_topics=max_topics)
            self.fit_transform()
            
            # 3. í‰ê°€ ì§€í‘œ ê³„ì‚°
            coherence_score = self.calculate_coherence()
            diversity_score = self.calculate_topic_diversity()
            similarity_matrix = self.create_similarity_matrix()
            
            # 4. í† í”½ ì •ë³´ ì¶”ì¶œ
            topic_details = self.extract_topic_keywords_and_examples()
            
            # 5. ê²°ê³¼ ì €ì¥
            self.save_results()
            
            # 6. ì‹œê°í™” ìƒì„±
            self.create_visualizations()
            
            print("\n" + "=" * 50)
            print("âœ… BERTopic ë¶„ì„ ì™„ë£Œ!")
            print("=" * 50)
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
            print(f"  - ì´ ë¬¸ì„œ ìˆ˜: {len(self.documents)}")
            print(f"  - ë°œê²¬ëœ í† í”½ ìˆ˜: {len(set(self.topics)) - (1 if -1 in self.topics else 0)}")
            if max_topics:
                print(f"  - ìµœëŒ€ í† í”½ ìˆ˜ ì œí•œ: {max_topics}")
            if coherence_score:
                print(f"  - Coherence Score: {coherence_score:.4f}")
            if diversity_score:
                print(f"  - Topic Diversity: {diversity_score:.4f}")
            
            return self.results
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def run_analysis_with_fixed_topics(self, data_path: str, n_topics: int, text_column: str = 'cleaned_text') -> Dict[str, Any]:
        """
        ì •í™•í•œ í† í”½ ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ BERTopic ë¶„ì„ ì‹¤í–‰ (KMeans ì‚¬ìš©)
        
        Args:
            data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            n_topics: ìƒì„±í•  í† í”½ ìˆ˜ (ì •í™•íˆ ì´ ê°œìˆ˜ë§Œí¼ ìƒì„±ë¨)
            text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
        """
        print("ğŸš€ ê³ ì • í† í”½ ìˆ˜ BERTopic ë¶„ì„ ì‹œì‘...")
        print("=" * 50)
        print(f"ğŸ“Š ì§€ì •ëœ í† í”½ ìˆ˜: {n_topics}ê°œ")
        print("=" * 50)
        
        # ê³ ì • í† í”½ ìˆ˜ ì €ì¥ (íŒŒì¼ëª…ì— ì‚¬ìš©)
        self.fixed_topics = n_topics
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            df = self.load_data(data_path, text_column)
            
            # 2. KMeans ê¸°ë°˜ ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ
            self.setup_model_with_kmeans(n_topics=n_topics)
            self.fit_transform()
            
            # 3. í‰ê°€ ì§€í‘œ ê³„ì‚° (í™•ë¥ ì´ ì—†ìœ¼ë¯€ë¡œ ì¼ë¶€ ê±´ë„ˆë›°ê¸°)
            coherence_score = self.calculate_coherence()
            diversity_score = self.calculate_topic_diversity()
            similarity_matrix = self.create_similarity_matrix()
            
            # 4. í† í”½ ì •ë³´ ì¶”ì¶œ
            topic_details = self.extract_topic_keywords_and_examples()
            
            # 5. ê²°ê³¼ ì €ì¥
            self.save_results()
            
            # 6. ì‹œê°í™” ìƒì„±
            self.create_visualizations()
            
            print("\n" + "=" * 50)
            print("âœ… ê³ ì • í† í”½ ìˆ˜ BERTopic ë¶„ì„ ì™„ë£Œ!")
            print("=" * 50)
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
            print(f"  - ì´ ë¬¸ì„œ ìˆ˜: {len(self.documents)}")
            print(f"  - ìƒì„±ëœ í† í”½ ìˆ˜: {len(set(self.topics)) - (1 if -1 in self.topics else 0)}")
            print(f"  - ì§€ì •ëœ í† í”½ ìˆ˜: {n_topics}")
            if coherence_score:
                print(f"  - Coherence Score: {coherence_score:.4f}")
            if diversity_score:
                print(f"  - Topic Diversity: {diversity_score:.4f}")
            
            return self.results
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def test_visualizations(self) -> None:
        """
        ì‹œê°í™” ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜
        """
        print("ğŸ§ª ì‹œê°í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        # ê¸°ë³¸ ìƒíƒœ í™•ì¸
        print(f"   topic_model: {self.topic_model is not None}")
        print(f"   topics: {self.topics is not None}")
        print(f"   documents: {len(self.documents) if self.documents else 0}")
        
        if self.topic_model is not None:
            try:
                topic_info = self.topic_model.get_topic_info()
                print(f"   í† í”½ ì •ë³´: {len(topic_info)}ê°œ í† í”½")
                print(f"   í† í”½ IDë“¤: {list(topic_info['Topic'].values)[:5]}...")  # ì²˜ìŒ 5ê°œë§Œ
            except Exception as e:
                print(f"   í† í”½ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        if self.topics is not None:
            unique_topics = set(self.topics)
            print(f"   í• ë‹¹ëœ í† í”½ë“¤: {len(unique_topics)}ê°œ")
            print(f"   ë…¸ì´ì¦ˆ ë¬¸ì„œ: {sum(1 for t in self.topics if t == -1)}ê°œ")
        
        # ì‹¤ì œ ì‹œê°í™” í…ŒìŠ¤íŠ¸
        self.create_visualizations()

    def load_existing_model_and_test_viz(self, model_path: str = None) -> None:
        """
        ê¸°ì¡´ì— í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì‹œê°í™”ë§Œ í…ŒìŠ¤íŠ¸
        """
        print("ğŸ“‚ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ë° ì‹œê°í™” í…ŒìŠ¤íŠ¸...")
        
        try:
            # ê°€ì¥ ìµœê·¼ ëª¨ë¸ ì°¾ê¸°
            if model_path is None:
                import glob
                model_dirs = glob.glob("Results/bertopic_model_*")
                if not model_dirs:
                    print("âŒ ê¸°ì¡´ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return
                model_path = sorted(model_dirs)[-1]  # ê°€ì¥ ìµœê·¼ ëª¨ë¸
            
            print(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
            
            # BERTopic ëª¨ë“ˆ ì„í¬íŠ¸
            BERTopic = import_bertopic()
            
            # ëª¨ë¸ ë¡œë“œ
            self.topic_model = BERTopic.load(model_path)
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # í† í”½ ì •ë³´ í™•ì¸
            topic_info = self.topic_model.get_topic_info()
            print(f"ğŸ“Š ë¡œë“œëœ í† í”½ ìˆ˜: {len(topic_info)}ê°œ")
            
            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í† í”½ í• ë‹¹ ì¬ìƒì„± ì‹œë„
            try:
                print("ğŸ”„ ê¸°ì¡´ ë°ì´í„°ë¡œ í† í”½ í• ë‹¹ ì¬ìƒì„± ì‹œë„...")
                data_path = "Results/pre_dataframe.xlsx"
                df = pd.read_excel(data_path)
                if 'cleaned_text' in df.columns:
                    self.documents = df['cleaned_text'].dropna().tolist()
                    print(f"   ğŸ“Š ë¬¸ì„œ ìˆ˜: {len(self.documents)}")
                    
                    # í† í”½ í• ë‹¹ ì¬ìƒì„±
                    self.topics = self.topic_model.transform(self.documents)[0]
                    print(f"   âœ… í† í”½ í• ë‹¹ ì¬ìƒì„± ì™„ë£Œ: {len(set(self.topics))}ê°œ í† í”½")
                else:
                    print("   âš ï¸ cleaned_text ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"   âš ï¸ í† í”½ í• ë‹¹ ì¬ìƒì„± ì‹¤íŒ¨: {e}")
                print("   ğŸ’¡ ëª¨ë¸ë§Œìœ¼ë¡œ ì‹œê°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # ì•ˆì „í•œ ì‹œê°í™” í…ŒìŠ¤íŠ¸
            print("\nğŸ§ª ì•ˆì „í•œ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
            self.create_safe_visualizations()
            
            # ì „ì²´ ì‹œê°í™” í…ŒìŠ¤íŠ¸
            print("\nğŸ§ª ì „ì²´ ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
            self.create_visualizations()
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ë° ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")


# === ë¶„ì„ ì „ Coherence & Diversity ê³„ì‚° ë° ìµœì  í† í”½ìˆ˜ ì¶”ì²œ ===
def calculate_coherence_and_diversity(texts, min_topics=2, max_topics=15):
    from gensim.corpora import Dictionary
    from gensim.models import CoherenceModel, LdaModel
    import numpy as np

    dictionary = Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    coherence_scores = []
    diversity_scores = []

    for num_topics in range(min_topics, max_topics+1):
        lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)
        coherence_model = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence = coherence_model.get_coherence()
        # Topic diversity: unique word ë¹„ìœ¨
        topic_words = [set([w for w, _ in lda.show_topic(i, topn=10)]) for i in range(num_topics)]
        all_words = set().union(*topic_words)
        diversity = len(all_words) / (num_topics * 10)
        coherence_scores.append(coherence)
        diversity_scores.append(diversity)

    return coherence_scores, diversity_scores

def find_optimal_topics(coherence_scores, min_topics=2):
    import numpy as np
    return int(np.argmax(coherence_scores)) + min_topics

def find_optimal_topics_bertopic(data_path, text_column='cleaned_text', min_topics=2, max_topics=8):
    """
    BERTopic ê¸°ë°˜ ìµœì  í† í”½ ìˆ˜(coherence ê¸°ì¤€) ì¶”ì²œ
    (ì£¼ì˜: ëŠë¦´ ìˆ˜ ìˆìŒ)
    """
    from time import time
    coherence_scores = []
    for n_topics in range(min_topics, max_topics+1):
        print(f"\n[BERTopic ìµœì  í† í”½ íƒìƒ‰] n_topics={n_topics} ë¶„ì„ ì¤‘...")
        analyzer = BERTopicAnalyzer()
        t0 = time()
        analyzer.run_analysis_with_fixed_topics(data_path, n_topics=n_topics, text_column=text_column)
        score = analyzer.calculate_coherence()
        elapsed = time() - t0
        print(f"n_topics={n_topics}, coherence={score:.4f} (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")
        coherence_scores.append(score)
    optimal_topics = coherence_scores.index(max(coherence_scores)) + min_topics
    print(f"\n[BERTopic ê¸°ë°˜] ìµœì  í† í”½ ìˆ˜: {optimal_topics} (coherence={max(coherence_scores):.4f})")
    return optimal_topics, coherence_scores

# === main í•¨ìˆ˜ ë˜ëŠ” ë¶„ì„ ì§„ì…ì ì— ì•„ë˜ ì½”ë“œ ì¶”ê°€ ===
if __name__ == "__main__":
    import os
    import pandas as pd
    import matplotlib.pyplot as plt

    print("==== [Step 1] pre_dataframe.xlsx ë¡œë“œ ====")
    if not os.path.exists("Results/pre_dataframe.xlsx"):
        print("âŒ Results/pre_dataframe.xlsx íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. preprocessing.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)
    df = pd.read_excel("Results/pre_dataframe.xlsx")
    if 'cleaned_text' not in df.columns:
        print("âŒ 'cleaned_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. preprocessing.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)
    texts = df['cleaned_text'].dropna().tolist()
    if len(texts) == 0:
        print("âŒ 'cleaned_text'ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocessing.pyë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        exit(1)
    print(f"âœ… cleaned_text ìƒ˜í”Œ: {texts[:3]}")
    print(f"âœ… ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(texts)}")

    # LDA ê¸°ë°˜ Coherence & Topic Diversity ê³¡ì„  ì‹œê°í™”
    print("\n==== [Step 2] LDA ê¸°ë°˜ Coherence & Topic Diversity ê³¡ì„  ì‹œê°í™” ====")
    min_topics, max_topics = 3, 8
    try:
        coherence_scores, diversity_scores = calculate_coherence_and_diversity([t.split() for t in texts], min_topics, max_topics)
        optimal_topics = 7  # ë˜ëŠ” ì›í•˜ëŠ” ê°’
        plt.figure(figsize=(10,5))
        plt.plot(range(min_topics, max_topics+1), coherence_scores, marker='o', label='Coherence')
        plt.plot(range(min_topics, max_topics+1), diversity_scores, marker='s', label='Diversity')
        plt.axvline(optimal_topics, color='red', linestyle='--', label=f'Optimal: {optimal_topics}')
        plt.xlabel('Number of Topics')
        plt.ylabel('Score')
        plt.title('Coherence & Topic Diversity')
        plt.legend()
        plt.tight_layout()
        os.makedirs('Results', exist_ok=True)
        plt.savefig('Results/lda_coherence_diversity_curve.png', dpi=200)
        plt.close()
        print("âœ… Results/lda_coherence_diversity_curve.png ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ LDA Coherence & Diversity ê³¡ì„  ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

    print("\n==== [Step 3] BERTopic ë¶„ì„ ====")
    analyzer = BERTopicAnalyzer(language_model="paraphrase-multilingual-MiniLM-L12-v2")
    analyzer.documents = texts
    analyzer.run_analysis_with_fixed_topics("Results/pre_dataframe.xlsx", n_topics=7)
