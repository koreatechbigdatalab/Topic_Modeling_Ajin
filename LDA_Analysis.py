#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LDA í† í”½ ëª¨ë¸ë§ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ì‘ì„±ì: ë² í…Œë‘ ë°ì´í„° ë¶„ì„ê°€
ëª©ì : AI ê´‘ê³  ë°ì´í„°ì˜ í† í”½ ë¶„ì„

ì‚¬ìš©ë²•:
======
1. ëŒ€í™”í˜• ëª¨ë“œ (ê¶Œì¥):
   python LDA_Analysis.py

2. ëª…ë ¹í–‰ ëª¨ë“œ:
   python LDA_Analysis.py --topics 5         # 5ê°œ í† í”½ìœ¼ë¡œ ë¶„ì„
   python LDA_Analysis.py --help             # ë„ì›€ë§ ë³´ê¸°
   
3. ì‚¬ìš©ì ì •ì˜ ë°ì´í„°:
   python LDA_Analysis.py --data custom.xlsx --column text_col --topics 8

âš ï¸ ì°¸ê³ : í† í”½ ìˆ˜ëŠ” í•­ìƒ ìˆ˜ë™ìœ¼ë¡œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤ (2-50 ê¶Œì¥)

ì£¼ìš” ë¶„ì„ í•­ëª©:
==============
1. LDA í† í”½ ëª¨ë¸ë§ (ìˆ˜ë™ í† í”½ ìˆ˜ ì§€ì •)
2. Coherence ì ìˆ˜ ê³„ì‚° (c_v, c_uci, c_npmi, u_mass)
3. Topic Diversity ì¸¡ì •
4. Similarity Matrix ìƒì„±
5. í† í”½ë³„ í‚¤ì›Œë“œ, ê°€ì¤‘ì¹˜, ì˜ˆì œ ë¬¸ì¥ ì¶”ì¶œ
6. ì‹œê°í™” ë° ì¢…í•© ë³´ê³ ì„œ ìƒì„±

ì¶œë ¥ íŒŒì¼ (í† í”½ ìˆ˜ í¬í•¨):
========================
- Excel ë¶„ì„ ê²°ê³¼: Results/lda_analysis_results_{í† í”½ìˆ˜}topics_*.xlsx
- í…ìŠ¤íŠ¸ ìš”ì•½: Results/lda_analysis_summary_{í† í”½ìˆ˜}topics_*.txt
- ì‹œê°í™” ì´ë¯¸ì§€ë“¤: Results/*_{í† í”½ìˆ˜}topics_*.png
- LDA ëª¨ë¸: Results/lda_model_{í† í”½ìˆ˜}topics_*
- ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤: Results/LDA_topic_similarity_matrix_{í† í”½ìˆ˜}topics_*.npy
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í† í”½ ëª¨ë¸ë§ ê´€ë ¨
from gensim import corpora, models
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel
import gensim.downloader as api

# ì „ì²˜ë¦¬ ë° ìœ í‹¸ë¦¬í‹°
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity
import re
from collections import Counter
import itertools
from datetime import datetime
import os
import argparse
import sys

# ì‹œê°í™” ì„¤ì •
# í•œêµ­ì–´ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œì— ë”°ë¼ ì¡°ì • í•„ìš”)
try:
    plt.rcParams['font.family'] = 'Malgun Gothic'
except:
    try:
        plt.rcParams['font.family'] = 'AppleGothic'
    except:
        plt.rcParams['font.family'] = 'DejaVu Sans'
        logger.warning("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")


plt.rcParams['font.size'] = 10
plt.rcParams['figure.figsize'] = (12, 8)


class LDAAnalyzer:
    """
    LDA í† í”½ ëª¨ë¸ë§ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, data_path="Results/pre_dataframe.xlsx", text_column="cleaned_text"):
        """
        ì´ˆê¸°í™”
        
        Args:
            data_path (str): ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            text_column (str): ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
        """
        self.data_path = data_path
        self.text_column = text_column
        self.df = None
        self.texts = None
        self.dictionary = None
        self.corpus = None
        self.lda_model = None
        self.num_topics = None  # í† í”½ ìˆ˜ ì €ì¥
        self.coherence_scores = {}
        self.topic_diversity = None
        self.similarity_matrix = None
        self.results = {}
        
    def load_data(self):
        """
        ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬
        """
        print("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
        try:
            self.df = pd.read_excel(self.data_path)
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ ë¬¸ì„œ")
            print(f"ğŸ“‹ ì»¬ëŸ¼: {list(self.df.columns)}")
            
            # cleaned_text ì»¬ëŸ¼ í™•ì¸
            if self.text_column not in self.df.columns:
                print(f"âŒ '{self.text_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(self.df.columns)}")
                return False
                
            # í…ìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
            self.df = self.df.dropna(subset=[self.text_column])
            self.df = self.df[self.df[self.text_column].str.len() > 10]  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œê±°
            
            print(f"âœ… ì „ì²˜ë¦¬ í›„ ë¬¸ì„œ ìˆ˜: {len(self.df)}ê°œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def preprocess_texts(self):
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”
        """
        print("ğŸ”§ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
        
        texts = self.df[self.text_column].tolist()
        
        # ê¸°ë³¸ ì „ì²˜ë¦¬
        processed_texts = []
        for text in texts:
            if pd.isna(text):
                continue
                
            # ë¬¸ìì—´ë¡œ ë³€í™˜
            text = str(text)
            
            # ê³µë°± ê¸°ì¤€ í† í°í™” (ì´ë¯¸ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¼ê³  ê°€ì •)
            tokens = text.split()
            
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ í† í° ì œê±°
            tokens = [token for token in tokens if 2 <= len(token) <= 15]
            
            if len(tokens) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒì˜ í† í°
                processed_texts.append(tokens)
        
        self.texts = processed_texts
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(self.texts)}ê°œ ë¬¸ì„œ")
        
        # Gensim ì‚¬ì „ ë° ì½”í¼ìŠ¤ ìƒì„±
        self.dictionary = corpora.Dictionary(self.texts)
        
        # ë„ˆë¬´ ë¹ˆë²ˆí•˜ê±°ë‚˜ í¬ì†Œí•œ ë‹¨ì–´ ì œê±°
        self.dictionary.filter_extremes(no_below=5, no_above=0.7)
        
        # ì½”í¼ìŠ¤ ìƒì„±
        self.corpus = [self.dictionary.doc2bow(text) for text in self.texts]
        
        print(f"ğŸ“š ì‚¬ì „ í¬ê¸°: {len(self.dictionary)} ë‹¨ì–´")
        print(f"ğŸ“„ ì½”í¼ìŠ¤ í¬ê¸°: {len(self.corpus)} ë¬¸ì„œ")
    
    def find_optimal_topics(self, min_topics=2, max_topics=15):
        """
        ìµœì ì˜ í† í”½ ìˆ˜ ì°¾ê¸° (Coherence ê¸°ë°˜)
        
        Args:
            min_topics (int): ìµœì†Œ í† í”½ ìˆ˜
            max_topics (int): ìµœëŒ€ í† í”½ ìˆ˜
        """
        print("ğŸ” ìµœì  í† í”½ ìˆ˜ íƒìƒ‰ ì¤‘...")
        
        coherence_scores = []
        topic_nums = range(min_topics, max_topics + 1)
        
        for num_topics in topic_nums:
            print(f"   í† í”½ ìˆ˜ {num_topics} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # LDA ëª¨ë¸ í›ˆë ¨
            lda_model = LdaModel(
                corpus=self.corpus,
                id2word=self.dictionary,
                num_topics=num_topics,
                random_state=42,
                passes=20,
                alpha='auto',
                per_word_topics=True,
                eval_every=None
            )
            
            # Coherence ê³„ì‚°
            coherence_model = CoherenceModel(
                model=lda_model,
                texts=self.texts,
                dictionary=self.dictionary,
                coherence='c_v'
            )
            
            coherence_score = coherence_model.get_coherence()
            coherence_scores.append(coherence_score)
            self.coherence_scores[num_topics] = coherence_score
            
            print(f"   í† í”½ ìˆ˜ {num_topics}: Coherence = {coherence_score:.4f}")
        
        # ìµœì  í† í”½ ìˆ˜ ì„ íƒ
        optimal_topics = topic_nums[np.argmax(coherence_scores)]
        print(f"âœ… ìµœì  í† í”½ ìˆ˜: {optimal_topics} (Coherence: {max(coherence_scores):.4f})")
        
        # Coherence ì ìˆ˜ ì‹œê°í™”
        plt.figure(figsize=(10, 6))
        plt.plot(topic_nums, coherence_scores, 'bo-', linewidth=2, markersize=8)
        plt.xlabel('Topics', fontsize=12)
        plt.ylabel('Coherence Score', fontsize=12)
        plt.title('Coherence Score by Topics', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.xticks(topic_nums)
        
        # ìµœì ì  í‘œì‹œ
        max_idx = np.argmax(coherence_scores)
        plt.axvline(x=topic_nums[max_idx], color='red', linestyle='--', alpha=0.7)
        plt.axhline(y=max(coherence_scores), color='red', linestyle='--', alpha=0.7)
        # plt.text(topic_nums[max_idx], max(coherence_scores), 
        #         f'  ìµœì ì : {optimal_topics}í† í”½\n  Coherence: {max(coherence_scores):.4f}',
        #         fontsize=10, ha='left', va='bottom',
        #         bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
        
        plt.tight_layout()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plt.savefig(f'Results/LDA_coherence_optimization_{min_topics}to{max_topics}topics_{timestamp}.png', dpi=300, bbox_inches='tight')
        # plt.show()
        
        return optimal_topics
    
    def train_lda_model(self, num_topics):
        """
        LDA ëª¨ë¸ í›ˆë ¨
        
        Args:
            num_topics (int): í† í”½ ìˆ˜
        """
        if num_topics is None:
            raise ValueError("í† í”½ ìˆ˜ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.num_topics = num_topics  # í† í”½ ìˆ˜ ì €ì¥
        
        print(f"ğŸ¯ LDA ëª¨ë¸ í›ˆë ¨ ì¤‘... (í† í”½ ìˆ˜: {num_topics})")
        
        # LDA ëª¨ë¸ í›ˆë ¨
        self.lda_model = LdaModel(
            corpus=self.corpus,
            id2word=self.dictionary,
            num_topics=num_topics,
            random_state=42,
            passes=50,
            iterations=100,
            alpha='auto',
            eta='auto',
            per_word_topics=True,
            eval_every=None
        )
        
        print("âœ… LDA ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        
        # ëª¨ë¸ ì €ì¥ (í† í”½ ìˆ˜ í¬í•¨)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = f"Results/lda_model_{num_topics}topics_{timestamp}"
        self.lda_model.save(model_path)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")
    
    def calculate_coherence(self):
        """
        ë‹¤ì–‘í•œ Coherence ë©”íŠ¸ë¦­ ê³„ì‚°
        """
        print("ğŸ“Š Coherence ì ìˆ˜ ê³„ì‚° ì¤‘...")
        
        coherence_metrics = ['c_v', 'c_uci', 'c_npmi', 'u_mass']
        
        for metric in coherence_metrics:
            try:
                coherence_model = CoherenceModel(
                    model=self.lda_model,
                    texts=self.texts,
                    dictionary=self.dictionary,
                    coherence=metric
                )
                score = coherence_model.get_coherence()
                self.coherence_scores[metric] = score
                print(f"   {metric.upper()}: {score:.4f}")
            except Exception as e:
                print(f"   {metric.upper()}: ê³„ì‚° ì‹¤íŒ¨ ({str(e)})")
        
        return self.coherence_scores
    
    def calculate_topic_diversity(self):
        """
        Topic Diversity ê³„ì‚°
        í† í”½ ê°„ í‚¤ì›Œë“œ ê²¹ì¹¨ì„ ì¸¡ì •í•˜ì—¬ ë‹¤ì–‘ì„± í‰ê°€
        """
        print("ğŸŒˆ Topic Diversity ê³„ì‚° ì¤‘...")
        
        # ê° í† í”½ì˜ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        num_words = 20
        topic_words = []
        
        for topic_id in range(self.lda_model.num_topics):
            words = [word for word, _ in self.lda_model.show_topic(topic_id, num_words)]
            topic_words.append(set(words))
        
        # í† í”½ ê°„ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ê³„ì‚°
        unique_words = set()
        total_words = 0
        
        for words in topic_words:
            unique_words.update(words)
            total_words += len(words)
        
        # Topic Diversity ê³„ì‚° (ê²¹ì¹˜ì§€ ì•ŠëŠ” ë‹¨ì–´ì˜ ë¹„ìœ¨)
        self.topic_diversity = len(unique_words) / total_words
        
        print(f"âœ… Topic Diversity: {self.topic_diversity:.4f}")
        print(f"   ì „ì²´ ê³ ìœ  ë‹¨ì–´: {len(unique_words)}")
        print(f"   ì „ì²´ ë‹¨ì–´ (ì¤‘ë³µí¬í•¨): {total_words}")
        
        # í† í”½ ê°„ ë‹¨ì–´ ê²¹ì¹¨ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        overlap_matrix = np.zeros((self.lda_model.num_topics, self.lda_model.num_topics))
        
        for i in range(self.lda_model.num_topics):
            for j in range(self.lda_model.num_topics):
                if i != j:
                    intersection = len(topic_words[i].intersection(topic_words[j]))
                    union = len(topic_words[i].union(topic_words[j]))
                    overlap_matrix[i][j] = intersection / union if union > 0 else 0
        
        # ê²¹ì¹¨ ë§¤íŠ¸ë¦­ìŠ¤ ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.heatmap(overlap_matrix, 
                   annot=True, 
                   fmt='.3f',
                   cmap='Reds',
                   xticklabels=[f'Topic {i}' for i in range(self.lda_model.num_topics)],
                   yticklabels=[f'Topic {i}' for i in range(self.lda_model.num_topics)])
        plt.title('Overlap by Topics', 
                 fontsize=14, fontweight='bold')
        plt.tight_layout()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plt.savefig(f'Results/LDA_topic_overlap_matrix_{self.num_topics}topics_{timestamp}.png', dpi=300, bbox_inches='tight')
        # plt.show()
        
        return self.topic_diversity
    
    def calculate_similarity_matrix(self):
        """
        í† í”½ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        """
        print("ğŸ”— í† í”½ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì¤‘...")
        
        # í† í”½ ë¶„í¬ ì¶”ì¶œ
        topic_distributions = []
        
        for topic_id in range(self.lda_model.num_topics):
            # ê° í† í”½ì˜ ë‹¨ì–´ ë¶„í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
            topic_words = dict(self.lda_model.show_topic(topic_id, len(self.dictionary)))
            
            # ì „ì²´ ë‹¨ì–´ì— ëŒ€í•œ í™•ë¥  ë²¡í„° ìƒì„±
            topic_vector = np.zeros(len(self.dictionary))
            for word_id, word in self.dictionary.items():
                if word in topic_words:
                    topic_vector[word_id] = topic_words[word]
            
            topic_distributions.append(topic_vector)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        topic_distributions = np.array(topic_distributions)
        self.similarity_matrix = cosine_similarity(topic_distributions)
        
        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.heatmap(self.similarity_matrix,
                   annot=True,
                   fmt='.3f',
                   cmap='coolwarm',
                   center=0,
                   xticklabels=[f'Topic {i}' for i in range(self.lda_model.num_topics)],
                   yticklabels=[f'Topic {i}' for i in range(self.lda_model.num_topics)])
        plt.title('Topic Similarrity Matrix(Cosine Similarity)', 
                 fontsize=14, fontweight='bold')
        plt.tight_layout()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plt.savefig(f'Results/LDA_topic_similarity_matrix_{self.num_topics}topics_{timestamp}.png', dpi=300, bbox_inches='tight')
        # plt.show()
        
        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥
        np.save(f'Results/LDA_topic_similarity_matrix_{self.num_topics}topics_{timestamp}.npy', self.similarity_matrix)
        
        return self.similarity_matrix
    
    def extract_topic_information(self):
        """
        í† í”½ë³„ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        - í‚¤ì›Œë“œ 10ê°œì™€ ê°€ì¤‘ì¹˜
        - í•´ì„ì„ ìœ„í•œ ì˜ˆì œ ë¬¸ì¥
        """
        print("ğŸ“ í† í”½ë³„ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘...")
        
        topic_info = []
        
        for topic_id in range(self.lda_model.num_topics):
            print(f"   í† í”½ {topic_id} ë¶„ì„ ì¤‘...")
            
            # í† í”½ì˜ ìƒìœ„ í‚¤ì›Œë“œ 10ê°œì™€ ê°€ì¤‘ì¹˜
            topic_words = self.lda_model.show_topic(topic_id, 10)
            keywords = [word for word, weight in topic_words]
            weights = [weight for word, weight in topic_words]
            
            # í•´ë‹¹ í† í”½ì— ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ ì°¾ê¸°
            doc_topic_probs = []
            for doc_idx, doc in enumerate(self.corpus):
                doc_topics = self.lda_model.get_document_topics(doc)
                topic_prob = 0
                for t_id, prob in doc_topics:
                    if t_id == topic_id:
                        topic_prob = prob
                        break
                doc_topic_probs.append((doc_idx, topic_prob))
            
            # ìƒìœ„ í™•ë¥  ë¬¸ì„œë“¤ ì„ íƒ (ìƒìœ„ 5ê°œ)
            doc_topic_probs.sort(key=lambda x: x[1], reverse=True)
            top_docs = doc_topic_probs[:5]
            
            # ì˜ˆì œ ë¬¸ì¥ ì¶”ì¶œ
            example_sentences = []
            for doc_idx, prob in top_docs:
                if doc_idx < len(self.df):
                    sentence = str(self.df.iloc[doc_idx][self.text_column])
                    example_sentences.append({
                        'sentence': sentence[:200] + '...' if len(sentence) > 200 else sentence,
                        'probability': prob
                    })
            
            # í† í”½ í•´ì„ ìƒì„± (í‚¤ì›Œë“œ ê¸°ë°˜)
            interpretation = self.generate_topic_interpretation(keywords)
            
            topic_info.append({
                'topic_id': topic_id,
                'keywords': keywords,
                'weights': weights,
                'interpretation': interpretation,
                'example_sentences': example_sentences,
                'total_documents': len([prob for _, prob in doc_topic_probs if prob > 0.1])
            })
        
        self.results['topic_information'] = topic_info
        return topic_info
    
    def generate_topic_interpretation(self, keywords):
        """
        í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ í† í”½ í•´ì„ ìƒì„±
        
        Args:
            keywords (list): í† í”½ì˜ ì£¼ìš” í‚¤ì›Œë“œë“¤
            
        Returns:
            str: í† í”½ í•´ì„
        """
        # í‚¤ì›Œë“œ ë¶„ì„ì„ í†µí•œ ê°„ë‹¨í•œ í•´ì„ ìƒì„±
        keyword_str = ', '.join(keywords[:5])
        
        # ê´‘ê³  ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´ ë¶„ì„
        ad_patterns = {
            'ë¸Œëœë“œ': ['ë¸Œëœë“œ', 'ê¸°ì—…', 'íšŒì‚¬', 'ë§ˆì¼€íŒ…'],
            'ê¸°ìˆ ': ['AI', 'ì¸ê³µì§€ëŠ¥', 'ê¸°ìˆ ', 'ë””ì§€í„¸', 'ë°ì´í„°'],
            'ì†Œë¹„ì': ['ê³ ê°', 'ì†Œë¹„ì', 'ì‚¬ìš©ì', 'ì‚¬ëŒë“¤'],
            'íš¨ê³¼': ['íš¨ê³¼', 'ì„±ê³¼', 'ê²°ê³¼', 'ì„±ê³µ'],
            'ë¯¸ë””ì–´': ['ë¯¸ë””ì–´', 'ê´‘ê³ ', 'ì½˜í…ì¸ ', 'ì±„ë„'],
            'ì œí’ˆ': ['ì œí’ˆ', 'ì„œë¹„ìŠ¤', 'ì†”ë£¨ì…˜']
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        matched_categories = []
        for category, patterns in ad_patterns.items():
            if any(pattern in keyword_str for pattern in patterns):
                matched_categories.append(category)
        
        if matched_categories:
            interpretation = f"ì´ í† í”½ì€ ì£¼ë¡œ {', '.join(matched_categories)} ê´€ë ¨ ë‚´ìš©ì„ ë‹¤ë£¨ë©°, "
        else:
            interpretation = "ì´ í† í”½ì€ "
            
        interpretation += f"í•µì‹¬ í‚¤ì›Œë“œëŠ” {keyword_str} ë“±ì…ë‹ˆë‹¤."
        
        return interpretation
    
    def create_comprehensive_report(self):
        """
        ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
        """
        print("ğŸ“‹ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. í† í”½ë³„ ìƒì„¸ ì •ë³´ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        topic_details = []
        for topic in self.results['topic_information']:
            topic_details.append({
                'í† í”½ID': topic['topic_id'],
                'í•´ì„': topic['interpretation'],
                'í‚¤ì›Œë“œ': ', '.join(topic['keywords']),
                'í‚¤ì›Œë“œ_ê°€ì¤‘ì¹˜': ', '.join([f"{w:.4f}" for w in topic['weights']]),
                'ê´€ë ¨ë¬¸ì„œìˆ˜': topic['total_documents'],
                'ì˜ˆì œë¬¸ì¥1': topic['example_sentences'][0]['sentence'] if topic['example_sentences'] else '',
                'ì˜ˆì œë¬¸ì¥1_í™•ë¥ ': topic['example_sentences'][0]['probability'] if topic['example_sentences'] else 0,
                'ì˜ˆì œë¬¸ì¥2': topic['example_sentences'][1]['sentence'] if len(topic['example_sentences']) > 1 else '',
                'ì˜ˆì œë¬¸ì¥2_í™•ë¥ ': topic['example_sentences'][1]['probability'] if len(topic['example_sentences']) > 1 else 0
            })
        
        topic_df = pd.DataFrame(topic_details)
        
        # 2. Coherence ì ìˆ˜ DataFrame
        coherence_df = pd.DataFrame([
            {'ë©”íŠ¸ë¦­': k, 'ì ìˆ˜': v} for k, v in self.coherence_scores.items()
        ])
        
        # 3. ë¶„ì„ ìš”ì•½ ì •ë³´
        summary_info = {
            'ë¶„ì„ì¼ì‹œ': timestamp,
            'ì „ì²´ë¬¸ì„œìˆ˜': len(self.df),
            'í† í”½ìˆ˜': self.lda_model.num_topics,
            'ì‚¬ì „í¬ê¸°': len(self.dictionary),
            'Topic_Diversity': self.topic_diversity,
            'ìµœê³ _Coherence': max([v for k, v in self.coherence_scores.items() if isinstance(v, (int, float))])
        }
        
        summary_df = pd.DataFrame([summary_info])
        
        # 4. Excel íŒŒì¼ë¡œ ì €ì¥
        excel_path = f'Results/lda_analysis_results_{self.num_topics}topics_{timestamp}.xlsx'
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            summary_df.to_excel(writer, sheet_name='ë¶„ì„ìš”ì•½', index=False)
            topic_df.to_excel(writer, sheet_name='í† í”½ìƒì„¸ì •ë³´', index=False)
            coherence_df.to_excel(writer, sheet_name='Coherenceì ìˆ˜', index=False)
            
            # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ë„ ì €ì¥
            if self.similarity_matrix is not None:
                similarity_df = pd.DataFrame(
                    self.similarity_matrix,
                    columns=[f'í† í”½{i}' for i in range(self.lda_model.num_topics)],
                    index=[f'í† í”½{i}' for i in range(self.lda_model.num_topics)]
                )
                similarity_df.to_excel(writer, sheet_name='ìœ ì‚¬ë„ë§¤íŠ¸ë¦­ìŠ¤')
        
        print(f"âœ… ì¢…í•© ë³´ê³ ì„œ ì €ì¥: {excel_path}")
        
        # 5. í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ
        report_text = f"""
LDA í† í”½ ëª¨ë¸ë§ ë¶„ì„ ë³´ê³ ì„œ
============================

ë¶„ì„ ì¼ì‹œ: {timestamp}
ë°ì´í„° íŒŒì¼: {self.data_path}

1. ê¸°ë³¸ ì •ë³´
-----------
- ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(self.df):,}ê°œ
- í† í”½ ìˆ˜: {self.lda_model.num_topics}ê°œ
- ì‚¬ì „ í¬ê¸°: {len(self.dictionary):,}ê°œ ë‹¨ì–´
- Topic Diversity: {self.topic_diversity:.4f}

2. Coherence ì ìˆ˜
----------------
"""
        
        for metric, score in self.coherence_scores.items():
            if isinstance(score, (int, float)):
                report_text += f"- {metric.upper()}: {score:.4f}\n"
        
        report_text += f"""
3. í† í”½ë³„ ìƒì„¸ ì •ë³´
------------------
"""
        
        for i, topic in enumerate(self.results['topic_information']):
            report_text += f"""
í† í”½ {i}: {topic['interpretation']}
í‚¤ì›Œë“œ: {', '.join(topic['keywords'][:5])}
ê´€ë ¨ ë¬¸ì„œ ìˆ˜: {topic['total_documents']}ê°œ
ì˜ˆì œ ë¬¸ì¥: "{topic['example_sentences'][0]['sentence'] if topic['example_sentences'] else 'N/A'}"
"""
        
        # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥
        report_path = f'Results/lda_analysis_summary_{self.num_topics}topics_{timestamp}.txt'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"âœ… í…ìŠ¤íŠ¸ ìš”ì•½ ì €ì¥: {report_path}")
        
        return excel_path, report_path
    
    def visualize_topics(self):
        """
        í† í”½ ì‹œê°í™”
        """
        print("ğŸ“Š í† í”½ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # 1. í† í”½ë³„ í‚¤ì›Œë“œ ì¤‘ìš”ë„ ì‹œê°í™”
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for topic_id in range(min(6, self.lda_model.num_topics)):
            topic_words = self.lda_model.show_topic(topic_id, 10)
            words, weights = zip(*topic_words)
            
            axes[topic_id].barh(range(len(words)), weights)
            axes[topic_id].set_yticks(range(len(words)))
            axes[topic_id].set_yticklabels(words)
            axes[topic_id].set_title(f'Topic {topic_id}', fontweight='bold')
            axes[topic_id].invert_yaxis()
        
        # ë¹ˆ subplot ì œê±°
        for i in range(self.lda_model.num_topics, len(axes)):
            fig.delaxes(axes[i])
        
        plt.suptitle('Topic Keywords (Top10)', fontsize=16, fontweight='bold')
        plt.tight_layout()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plt.savefig(f'Results/LDA_topic_keywords_visualization_{self.num_topics}topics_{timestamp}.png', dpi=300, bbox_inches='tight')
        # plt.show()
        
        # 2. í† í”½ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        doc_topics = []
        for doc in self.corpus:
            doc_topic_dist = self.lda_model.get_document_topics(doc)
            doc_topic_dict = {topic_id: 0 for topic_id in range(self.lda_model.num_topics)}
            for topic_id, prob in doc_topic_dist:
                doc_topic_dict[topic_id] = prob
            doc_topics.append(doc_topic_dict)
        
        topic_counts = {i: 0 for i in range(self.lda_model.num_topics)}
        for doc_topic in doc_topics:
            dominant_topic = max(doc_topic, key=doc_topic.get)
            if doc_topic[dominant_topic] > 0.3:  # 30% ì´ìƒì¼ ë•Œë§Œ í•´ë‹¹ í† í”½ìœ¼ë¡œ ë¶„ë¥˜
                topic_counts[dominant_topic] += 1
        
        plt.figure(figsize=(12, 6))
        topics = list(topic_counts.keys())
        counts = list(topic_counts.values())
        
        bars = plt.bar([f'Topic {i}' for i in topics], counts, color='skyblue', alpha=0.7)
        plt.title('Distribution of documents by topic', fontsize=14, fontweight='bold')
        plt.xlabel('Topics', fontsize=12)
        plt.ylabel('Number of documents', fontsize=12)
        plt.xticks(rotation=45)
        
        # ë§‰ëŒ€ ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ
        for bar, count in zip(bars, counts):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    str(count), ha='center', va='bottom')
        
        plt.tight_layout()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        plt.savefig(f'Results/LDA_topic_distribution_{self.num_topics}topics_{timestamp}.png', dpi=300, bbox_inches='tight')
        # plt.show()
    
    def run_complete_analysis(self, num_topics):
        """
        ì „ì²´ LDA ë¶„ì„ ì‹¤í–‰
        
        Args:
            num_topics (int): í† í”½ ìˆ˜ (í•„ìˆ˜)
        """
        if num_topics is None:
            raise ValueError("í† í”½ ìˆ˜ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        print("ğŸš€ LDA í† í”½ ëª¨ë¸ë§ ë¶„ì„ ì‹œì‘")
        print("=" * 50)
        
        # 1. ë°ì´í„° ë¡œë“œ
        if not self.load_data():
            return False
        
        # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        self.preprocess_texts()
        
        # 3. LDA ëª¨ë¸ í›ˆë ¨
        self.train_lda_model(num_topics)
        
        # 4. Coherence ê³„ì‚°
        self.calculate_coherence()
        
        # 5. Topic Diversity ê³„ì‚°
        self.calculate_topic_diversity()
        
        # 6. ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        self.calculate_similarity_matrix()
        
        # 7. í† í”½ë³„ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        self.extract_topic_information()
        
        # 8. ì‹œê°í™”
        self.visualize_topics()
        
        # 9. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        excel_path, report_path = self.create_comprehensive_report()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ LDA ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²°ê³¼ íŒŒì¼: {excel_path}")
        print(f"ğŸ“ ìš”ì•½ ë³´ê³ ì„œ: {report_path}")
        print(f"ğŸ¯ í† í”½ ìˆ˜: {self.num_topics}")
        print(f"ğŸ“ˆ Topic Diversity: {self.topic_diversity:.4f}")
        print(f"ğŸ” ìµœê³  Coherence: {max([v for k, v in self.coherence_scores.items() if isinstance(v, (int, float))]):.4f}")
        
        return True


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì„œ ì„¤ì •
    parser = argparse.ArgumentParser(
        description='LDA í† í”½ ëª¨ë¸ë§ ë¶„ì„ ë„êµ¬ (ìˆ˜ë™ í† í”½ ìˆ˜ ì§€ì •)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python LDA_Analysis.py                    # ëŒ€í™”í˜• ëª¨ë“œ (í† í”½ ìˆ˜ ìˆ˜ë™ ì§€ì •)
  python LDA_Analysis.py --topics 5         # 5ê°œ í† í”½ìœ¼ë¡œ ë¶„ì„
  python LDA_Analysis.py --data custom.xlsx --topics 8  # ì‚¬ìš©ì ì •ì˜ ë°ì´í„° íŒŒì¼
        """
    )
    
    parser.add_argument(
        '--topics', '-t',
        type=int,
        help='í† í”½ ìˆ˜ ì§€ì • (2-50 ê¶Œì¥, í•„ìˆ˜)',
        metavar='N'
    )
    
    parser.add_argument(
        '--auto', '-a',
        action='store_true',
        help='âš ï¸ ì§€ì›ë˜ì§€ ì•ŠìŒ - ìˆ˜ë™ìœ¼ë¡œ í† í”½ ìˆ˜ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”'
    )
    
    parser.add_argument(
        '--data', '-d',
        type=str,
        default="Results/pre_dataframe.xlsx",
        help='ë¶„ì„í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: Results/pre_dataframe.xlsx)',
        metavar='FILE'
    )
    
    parser.add_argument(
        '--column', '-c',
        type=str,
        default="cleaned_text",
        help='ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: cleaned_text)',
        metavar='COLUMN'
    )
    
    args = parser.parse_args()
    
    print("ğŸ”¬ LDA í† í”½ ëª¨ë¸ë§ ë¶„ì„ ë„êµ¬")
    print("ì‘ì„±ì: ë² í…Œë‘ ë°ì´í„° ë¶„ì„ê°€ (10ë…„+ ê²½í—˜)")
    print("=" * 60)
    
    # Results ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
    if not os.path.exists('Results'):
        os.makedirs('Results')
        print("ğŸ“ Results ë””ë ‰í† ë¦¬ ìƒì„±")
    
    # í† í”½ ìˆ˜ ê²°ì •
    num_topics = None
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ê°€ ìˆëŠ” ê²½ìš°
    if args.auto:
        print("âš ï¸ ìë™ ìµœì í™” ëª¨ë“œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ í† í”½ ìˆ˜ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.")
        while True:
            try:
                num_topics = int(input("ğŸ”¢ í† í”½ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (2-20 ê¶Œì¥): "))
                if 2 <= num_topics <= 50:
                    print(f"âœ… í† í”½ ìˆ˜ë¥¼ {num_topics}ê°œë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
                    break
                else:
                    print("âš ï¸ í† í”½ ìˆ˜ëŠ” 2-50 ì‚¬ì´ì˜ ê°’ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            except ValueError:
                print("âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ë¶„ì„ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return
        
    elif args.topics:
        if 2 <= args.topics <= 50:
            print(f"âœ… í† í”½ ìˆ˜ {args.topics}ê°œë¡œ ì„¤ì • (ëª…ë ¹í–‰ ì˜µì…˜)")
            num_topics = args.topics
        else:
            print("âŒ í† í”½ ìˆ˜ëŠ” 2-50 ì‚¬ì´ì˜ ê°’ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            while True:
                try:
                    num_topics = int(input("ğŸ”¢ í† í”½ ìˆ˜ë¥¼ ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš” (2-20 ê¶Œì¥): "))
                    if 2 <= num_topics <= 50:
                        print(f"âœ… í† í”½ ìˆ˜ë¥¼ {num_topics}ê°œë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
                        break
                    else:
                        print("âš ï¸ í† í”½ ìˆ˜ëŠ” 2-50 ì‚¬ì´ì˜ ê°’ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
                except ValueError:
                    print("âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                except KeyboardInterrupt:
                    print("\nğŸ‘‹ ë¶„ì„ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                    return
            
    # ëŒ€í™”í˜• ëª¨ë“œ - í•­ìƒ ìˆ˜ë™ ì§€ì •
    else:
        print("\nğŸ¯ í† í”½ ìˆ˜ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”:")
        while True:
            try:
                num_topics = int(input("ğŸ”¢ í† í”½ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (2-20 ê¶Œì¥): "))
                if 2 <= num_topics <= 50:
                    print(f"âœ… í† í”½ ìˆ˜ë¥¼ {num_topics}ê°œë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
                    break
                else:
                    print("âš ï¸ í† í”½ ìˆ˜ëŠ” 2-50 ì‚¬ì´ì˜ ê°’ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            except ValueError:
                print("âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ë¶„ì„ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì¶œë ¥
    if args.data != "Results/pre_dataframe.xlsx":
        print(f"ğŸ“‚ ì‚¬ìš©ì ì •ì˜ ë°ì´í„° íŒŒì¼: {args.data}")
    if args.column != "cleaned_text":
        print(f"ğŸ“ ì‚¬ìš©ì ì •ì˜ í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {args.column}")
    
    # LDA ë¶„ì„ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
    analyzer = LDAAnalyzer(data_path=args.data, text_column=args.column)
    
    try:
        print(f"\nğŸš€ {num_topics}ê°œ í† í”½ìœ¼ë¡œ LDA ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
        success = analyzer.run_complete_analysis(num_topics)
        
        if success:
            print("\nâœ… ëª¨ë“  ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"\nğŸ“‹ ìƒì„±ëœ íŒŒì¼ë“¤ ({num_topics}ê°œ í† í”½):")
            print(f"- Excel ë¶„ì„ ê²°ê³¼: Results/lda_analysis_results_{num_topics}topics_*.xlsx")
            print(f"- í…ìŠ¤íŠ¸ ìš”ì•½: Results/lda_analysis_summary_{num_topics}topics_*.txt")
            print(f"- ì‹œê°í™” ì´ë¯¸ì§€ë“¤: Results/*_{num_topics}topics_*.png")
            print(f"- LDA ëª¨ë¸: Results/lda_model_{num_topics}topics_*")
            print(f"- ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤: Results/LDA_topic_similarity_matrix_{num_topics}topics_*.npy")
            
            print("\nğŸ’¡ ì‚¬ìš© íŒ:")
            print("- ë‹¤ìŒ ë²ˆì—ëŠ” ëª…ë ¹í–‰ ì˜µì…˜ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”:")
            print(f"  python LDA_Analysis.py --topics {num_topics}")
        else:
            print("\nâŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
